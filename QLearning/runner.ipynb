{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#----------ESSENTIAL_IMPORTS-----------------\n",
    "from config import *\n",
    "from model import *\n",
    "from utils import *\n",
    "from DopeTech import *\n",
    "#--------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started learning\n",
      "episode - 0\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 1\n",
      "totalDisqountedReward = 2.616738\n",
      "pureRewardPerGame  = 1.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 2\n",
      "totalDisqountedReward = 0.912400\n",
      "pureRewardPerGame  = 2.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 3\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 4.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 4\n",
      "totalDisqountedReward = 0.484887\n",
      "pureRewardPerGame  = 3.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 5\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 6\n",
      "totalDisqountedReward = 0.665159\n",
      "pureRewardPerGame  = 2.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 7\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 8\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 9\n",
      "totalDisqountedReward = 0.000977\n",
      "pureRewardPerGame  = 2.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 10\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 11\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 12\n",
      "totalDisqountedReward = 3.245525\n",
      "pureRewardPerGame  = 2.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 13\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 2.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 14\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 15\n",
      "totalDisqountedReward = 2.363913\n",
      "pureRewardPerGame  = 3.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 16\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 17\n",
      "totalDisqountedReward = 2.616738\n",
      "pureRewardPerGame  = 1.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 18\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 19\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 2.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 20\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 1.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 21\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 22\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 2.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 23\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 3.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 24\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 25\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 26\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 27\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 28\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 29\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 2.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 30\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 31\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 2.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 32\n",
      "totalDisqountedReward = 0.042975\n",
      "pureRewardPerGame  = 1.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 33\n",
      "totalDisqountedReward = 0.000001\n",
      "pureRewardPerGame  = 1.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 34\n",
      "totalDisqountedReward = 0.821160\n",
      "pureRewardPerGame  = 3.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 35\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 2.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 36\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 2.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 37\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 38\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 3.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 39\n",
      "totalDisqountedReward = 0.538764\n",
      "pureRewardPerGame  = 3.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 40\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 41\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 42\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 43\n",
      "totalDisqountedReward = 0.031329\n",
      "pureRewardPerGame  = 1.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 44\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 45\n",
      "totalDisqountedReward = 0.000002\n",
      "pureRewardPerGame  = 2.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 46\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 47\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 2.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 48\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 49\n",
      "totalDisqountedReward = 0.000020\n",
      "pureRewardPerGame  = 4.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 50\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 1.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 51\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 52\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 53\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 54\n",
      "totalDisqountedReward = 2.119558\n",
      "pureRewardPerGame  = 1.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 55\n",
      "totalDisqountedReward = 1.922587\n",
      "pureRewardPerGame  = 3.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 56\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 3.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 57\n",
      "totalDisqountedReward = 6.789125\n",
      "pureRewardPerGame  = 4.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 58\n",
      "totalDisqountedReward = 0.081329\n",
      "pureRewardPerGame  = 2.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 59\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 60\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 61\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 62\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 63\n",
      "totalDisqountedReward = 0.484887\n",
      "pureRewardPerGame  = 3.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 64\n",
      "totalDisqountedReward = 2.626570\n",
      "pureRewardPerGame  = 2.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 65\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 66\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 67\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 0.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 68\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 1.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 69\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 2.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 70\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 1.000000\n",
      "GameMemorySize = 50000\n",
      "\n",
      "episode - 71\n",
      "totalDisqountedReward = 0.000000\n",
      "pureRewardPerGame  = 1.000000\n",
      "GameMemorySize = 50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def mainCycle():\n",
    "    \"\"\"\n",
    "        Initialising two nets\n",
    "        first one - key net, which we are training\n",
    "        second one - net, that we use to estimate Q-value-function\n",
    "        for next states for Bellman Equation\n",
    "        \"\"\"\n",
    "    #--------------------------------------------\n",
    "    #keyNet = QModel().to(DEVICE)\n",
    "    keyNet = loadFromFile(\"QNet.pkl\")\n",
    "    helperNet = QModel().to(DEVICE)\n",
    "    helperNet.load_state_dict(keyNet.state_dict())\n",
    "    helperNet.eval()\n",
    "    #--------------------------------------------\n",
    "    optimizer = optim.Adam(keyNet.parameters(), lr = 1e-4)\n",
    "    gameMemory = GameMemory(50000)\n",
    "    ENVIRONMENT.render(mode = 'rgb_array')\n",
    "    \n",
    "    \n",
    "    #fillGameMemoryWithRandomTransitions(gameMemory)\n",
    "    #saveToFile(gameMemory, \"gameMemory.pkl\")\n",
    "    gameMemory = loadFromFile(\"gameMemory.pkl\")\n",
    "    stepsDone = 0\n",
    "    normalAction = lambda state: keyNet(state).max(1)[1].view(1, 1)\n",
    "    stateHolder = OneStateHolder()\n",
    "    \n",
    "    \n",
    "    print(\"started learning\")\n",
    "    for e in range(AMOUNT_OF_EPISODES):\n",
    "        #progressBar = myProgressBar(5)\n",
    "        currentLifes = 5\n",
    "        pureRewardPerGame = 0\n",
    "        totalDisqountedReward = 0\n",
    "        ENVIRONMENT.reset()\n",
    "        stateHolder.initWithFirstScreens()\n",
    "        isDone = False\n",
    "        while not isDone:\n",
    "            #performing action choosing according to epsilon greedy rule\n",
    "            ENVIRONMENT.render(mode = \"rgb_array\")\n",
    "            action = epsilonGreedyChooser(normalAction, stateHolder.getState().unsqueeze(0), stepsDone)\n",
    "            stepsDone += 1\n",
    "            screen, reward, isDone, info = ENVIRONMENT.step(action)\n",
    "            pureRewardPerGame += reward\n",
    "            reward = calculateRewardWithInfoGiven(reward, info, isDone)\n",
    "            stateHolder.pushScreen(screen)\n",
    "            \n",
    "            \n",
    "            if info['ale.lives'] < currentLifes:\n",
    "                currentLifes -=1\n",
    "                gameMemory.pushScreenActionReward(screen, action, reward, True)\n",
    "            else:\n",
    "                gameMemory.pushScreenActionReward(screen, action, reward, isDone)\n",
    "                \n",
    "            totalDisqountedReward = reward + totalDisqountedReward * DISCOUNT_FACTOR\n",
    "            \n",
    "            #progressBar.update(5 - info['ale.lives'])\n",
    "            #performing neural network training on our replayMemory\n",
    "            statesBatch, actionsBatch, nextStatesBatch, rewardsBatch, terminalMask = gameMemory.getBatch()\n",
    "            currentQValues = keyNet(statesBatch).gather(1, actionsBatch.unsqueeze(1))\n",
    "            nextQValues = torch.zeros(BATCH_SIZE, device = DEVICE)\n",
    "            nextQValues = helperNet(nextStatesBatch).max(1)[0].detach()\n",
    "            nextQValues[terminalMask == 1] = 0\n",
    "            expectedQValues = rewardsBatch + nextQValues * DISCOUNT_FACTOR\n",
    "            expectedQValues = torch.tensor(expectedQValues).unsqueeze(1).to(DEVICE)\n",
    "            loss = F.smooth_l1_loss(currentQValues, expectedQValues)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for param in keyNet.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "            optimizer.step()\n",
    "        \n",
    "        if e % 1 == 0:\n",
    "            print(\"episode -\", e)\n",
    "            print(\"totalDisqountedReward = %f\" % (totalDisqountedReward))\n",
    "            print(\"pureRewardPerGame  = %f\" % (pureRewardPerGame ))\n",
    "            print(\"GameMemorySize = %d\" % (len(gameMemory)))\n",
    "            print()\n",
    "        saveToFile(keyNet, \"QNet.pkl\")\n",
    "        saveToFile(gameMemory, \"gameMemory.pkl\")\n",
    "        if e % HELPER_UPDATE == 0:\n",
    "            helperNet.load_state_dict(keyNet.state_dict())\n",
    "            helperNet.eval()\n",
    "\n",
    "\n",
    "\n",
    "mainCycle()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
